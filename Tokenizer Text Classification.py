# -*- coding: utf-8 -*-
"""latihan8_dicoding_latihan tokenization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O4MkAM6OpaEVhM1L-epUXG7zRX7km9FC
"""

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words= 15, oov_token='<OOV>')

teks = ['Saya suka programming',
        'Programming sangat menyenangkan!',
        'Machine Learning berbeda dengan pemrograman konvensional']

tokenizer.fit_on_texts(teks)

sequences = tokenizer.texts_to_sequences(teks)

print(tokenizer.word_index)

print(tokenizer.texts_to_sequences(['Saya suka programming']))
print(tokenizer.texts_to_sequences(['Saya suka belajar programming sejak SMP']))

from tensorflow.keras.preprocessing.sequence import pad_sequences

sequences_samapanjang = pad_sequences(sequences)

print(sequences_samapanjang)

sequences_samapanjang = pad_sequences(sequences,
                                      padding = 'post',
                                      maxlen = 5)

sequences_samapanjang = pad_sequences(sequences,
                                      padding = 'post',
                                      maxlen = 5,
                                      truncating='post')

